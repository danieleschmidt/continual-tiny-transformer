# continual-tiny-transformer
Memory-efficient continual learning for transformers that adds ZERO new parameters per task. Based on Amazon Research's breakthrough showing how to extend models to dozens of tasks without catastrophic forgetting or parameter bloat.
